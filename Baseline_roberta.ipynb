{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "d101decf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import utils\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from loss_functions import LabelSmoothingLoss\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "from model import BertBaselineClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "e6aa8268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the device\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize the model\n",
    "model = BertBaselineClassifier(weights_name='bert-base-cased',\n",
    "                               n_classes_=2,\n",
    "                               batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "539f7467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data\n",
    "data = pd.read_csv(\n",
    "    'C:/Users/xiuyu/nlu/data/AgreementDataset/agreement_dataset_valid_tweets.tsv', sep='\\t')\n",
    "X = data['text']\n",
    "y = data['label']\n",
    "confidence = data['agreement_level']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "5e73ad10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6443    .@JoeBiden included images of Kyle Rittenhouse...\n",
       "1620    @therecount @stefcutter @realDonaldTrump as a ...\n",
       "2820      He is another nut case. https://t.co/UeBcOGTmYL\n",
       "1643    @JoeBiden White supremacists are still a thoud...\n",
       "2877    @flinch04 @FOX13News It's well wishes. In a cr...\n",
       "                              ...                        \n",
       "3772         CRAZY is contagious! https://t.co/k2qcuu9YEj\n",
       "5191    RT @CharlesDonnor: Hell we gave em smallpox an...\n",
       "5226    RT @BenandBrackenRu: Simple, clear and easily ...\n",
       "5390    We only have one chance, and that is November ...\n",
       "860     @Rannirebel @RealLucyLawless Savannah confused...\n",
       "Name: text, Length: 5453, dtype: object"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "c4964473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6443    0\n",
       "1620    1\n",
       "2820    1\n",
       "1643    0\n",
       "2877    0\n",
       "       ..\n",
       "3772    0\n",
       "5191    1\n",
       "5226    0\n",
       "5390    0\n",
       "860     1\n",
       "Name: label, Length: 5453, dtype: int64"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "939ddf3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       1\n",
       "       ..\n",
       "6812    0\n",
       "6813    0\n",
       "6814    0\n",
       "6815    0\n",
       "6816    1\n",
       "Name: label, Length: 6817, dtype: int64"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['label']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "9f50de46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       A++\n",
       "1       A++\n",
       "2        A+\n",
       "3       A++\n",
       "4       A++\n",
       "       ... \n",
       "6812    A++\n",
       "6813     A+\n",
       "6814     A+\n",
       "6815    A++\n",
       "6816    A++\n",
       "Name: agreement_level, Length: 6817, dtype: object"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "241d503a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5453])\n",
      "torch.Size([5453, 327])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 16. Training loss did not improve more than tol=1e-05. Final error is 821.289023399353."
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertBaselineClassifier(\n",
       "\tbatch_size=4,\n",
       "\tmax_iter=1000,\n",
       "\teta=0.001,\n",
       "\toptimizer_class=<class 'torch.optim.adam.Adam'>,\n",
       "\tl2_strength=0,\n",
       "\tgradient_accumulation_steps=1,\n",
       "\tmax_grad_norm=None,\n",
       "\tvalidation_fraction=0.1,\n",
       "\tearly_stopping=False,\n",
       "\tn_iter_no_change=10,\n",
       "\twarm_start=False,\n",
       "\ttol=1e-05,\n",
       "\thidden_dim=50,\n",
       "\thidden_activation=Tanh())"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "smoothed_labels = utils.smooth_labels(y, confidence)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c18e707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\xiuyu\\anaconda3\\envs\\nlu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Users\\xiuyu\\anaconda3\\envs\\nlu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Users\\xiuyu\\anaconda3\\envs\\nlu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "report = pd.DataFrame(classification_report(y_test, predictions, digits=3, output_dict=True)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65dbe7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.715543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.834188</td>\n",
       "      <td>976.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>388.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.715543</td>\n",
       "      <td>0.715543</td>\n",
       "      <td>0.715543</td>\n",
       "      <td>0.715543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.357771</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.417094</td>\n",
       "      <td>1364.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.512001</td>\n",
       "      <td>0.715543</td>\n",
       "      <td>0.596897</td>\n",
       "      <td>1364.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.715543  1.000000  0.834188   976.000000\n",
       "1              0.000000  0.000000  0.000000   388.000000\n",
       "accuracy       0.715543  0.715543  0.715543     0.715543\n",
       "macro avg      0.357771  0.500000  0.417094  1364.000000\n",
       "weighted avg   0.512001  0.715543  0.596897  1364.000000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "bed4c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, RobertaModel,RobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "77d52a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaModel_(nn.Module):\n",
    "    config = RobertaConfig()\n",
    "    def __init__(self, weights_name, n_classes_, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(weights_name, num_labels= n_classes_ ,output_hidden_states=True,output_attentions=False)\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768) \n",
    "        self.dropout = torch.nn.Dropout(0.225)\n",
    "        self.classifier = torch.nn.Linear(768, 5)\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = x[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        return self.classifier(pooler)     \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "d4a29872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClassifier(TorchShallowNeuralClassifier):\n",
    "\n",
    "    def __init__(self, weights_name, n_classes_, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.weights_name = weights_name\n",
    "        self.n_classes_ = n_classes_\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(weights_name)\n",
    "\n",
    "    def build_graph(self):\n",
    "        # computation graph\n",
    "        # sets model in TorchShallowNeuralClassifier fit\n",
    "        return RobertaModel_(self.weights_name, self.n_classes_)\n",
    "\n",
    "# TODO: modify to use empirical distribution as labels as in Ex Machina: Personal Attacks Seen at Scale?\n",
    "    def build_dataset(self, X, y=None):\n",
    "        data = self.tokenizer.batch_encode_plus(\n",
    "            X,\n",
    "            max_length=None,\n",
    "            add_special_tokens=True,\n",
    "            padding='longest',\n",
    "            return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        if y is None:\n",
    "            dataset = torch.utils.data.TensorDataset(indices, mask)\n",
    "        else:\n",
    "            self.classes_ = sorted(set(y))\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "            y = [class2index[label] for label in y]\n",
    "            y = torch.tensor(y)\n",
    "            print(y.size())\n",
    "            print(indices.size())\n",
    "            dataset = torch.utils.data.TensorDataset(indices, mask, y)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    # TODO: Implement score() in case we adopt metrics proposed in http://www.kayur.org/papers/chi2021.pdf\n",
    "    def score(self, X, y, device=None):\n",
    "        \"\"\"\n",
    "        Uses macro-F1 as the score function.\n",
    "\n",
    "        This function can be used to evaluate models, but its primary\n",
    "        use is in cross-validation and hyperparameter tuning.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: np.array, shape `(n_examples, n_features)`\n",
    "\n",
    "        y: iterable, shape `len(n_examples)`\n",
    "            These can be the raw labels. They will converted internally\n",
    "            as needed. See `build_dataset`.\n",
    "\n",
    "        device: str or None\n",
    "            Allows the user to temporarily change the device used\n",
    "            during prediction. This is useful if predictions require a\n",
    "            lot of memory and so are better done on the CPU. After\n",
    "            prediction is done, the model is returned to `self.device`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "\n",
    "        \"\"\"\n",
    "        preds = self.predict(X, device=device)\n",
    "        return utils.safe_macro_f1(y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "f8a32174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model_2 = RobertaClassifier(weights_name='roberta-base',\n",
    "                               n_classes_=2,\n",
    "                               batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "f407e930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5453])\n",
      "torch.Size([5453, 282])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 13. Training loss did not improve more than tol=1e-05. Final error is 3255.478422820568."
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaClassifier(\n",
       "\tbatch_size=2,\n",
       "\tmax_iter=1000,\n",
       "\teta=0.001,\n",
       "\toptimizer_class=<class 'torch.optim.adam.Adam'>,\n",
       "\tl2_strength=0,\n",
       "\tgradient_accumulation_steps=1,\n",
       "\tmax_grad_norm=None,\n",
       "\tvalidation_fraction=0.1,\n",
       "\tearly_stopping=False,\n",
       "\tn_iter_no_change=10,\n",
       "\twarm_start=False,\n",
       "\ttol=1e-05,\n",
       "\thidden_dim=50,\n",
       "\thidden_activation=Tanh())"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "model_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "49ae153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.to_pickle('C:/Users/xiuyu/nlu/roberta.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "7902f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2 = model_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "a636df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\xiuyu\\anaconda3\\envs\\nlu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Users\\xiuyu\\anaconda3\\envs\\nlu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Users\\xiuyu\\anaconda3\\envs\\nlu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report_2 = pd.DataFrame(classification_report(y_test, predictions_2, digits=3, output_dict=True)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "d2aa2471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.715543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.834188</td>\n",
       "      <td>976.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>388.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.715543</td>\n",
       "      <td>0.715543</td>\n",
       "      <td>0.715543</td>\n",
       "      <td>0.715543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.357771</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.417094</td>\n",
       "      <td>1364.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.512001</td>\n",
       "      <td>0.715543</td>\n",
       "      <td>0.596897</td>\n",
       "      <td>1364.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.715543  1.000000  0.834188   976.000000\n",
       "1              0.000000  0.000000  0.000000   388.000000\n",
       "accuracy       0.715543  0.715543  0.715543     0.715543\n",
       "macro avg      0.357771  0.500000  0.417094  1364.000000\n",
       "weighted avg   0.512001  0.715543  0.596897  1364.000000"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196ae79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
